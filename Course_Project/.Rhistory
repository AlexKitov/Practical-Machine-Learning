training <- segmentationOriginal[, which(segmentationOriginal$Case == "Test")]
opwhich(segmentationOriginal$Case == "Test")
op<-which(segmentationOriginal$Case == "Test")
training <- segmentationOriginal[, op]
training <- segmentationOriginal[op,]
testing <- segmentationOriginal[which(segmentationOriginal$Case != "Test"),]
set.seed(125)
training <- training[,-2]
View(training)
testing <- testing[,-2]
modfit <- rpart(Case~.,)
require(rpart)
modfit <- rpart(Case~.,)
modfit <- rpart(Case~., data = segmentationOriginal)
printcp(modfit)
plotcp(fit)
fit <- modfit
plotcp(fit)
summary(fit)
sum<-summary(fit)
sum <- summary(fit)
fit <- rpart(Class~., data = segmentationOriginal)
printcp(fit)
plotcp(fit)
summary(fit)
plot(fit, uniform=TRUE,
main="Classification Tree for Kyphosis")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
fit <- rpart(Class~., data = training)
printcp(fit)
plotcp(fit)
plot(fit, uniform=TRUE,
main="Classification Tree for Kyphosis")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
plot(fit, uniform=TRUE,       main="Classification Tree for Kyphosis")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
modFit <- train(Class~. method="rpart", data=training)
modFit <- train(Class~. method = "rpart", data=training)
modFit <- train(Class~., method = "rpart", data=training)
print(modFit$finalModel)
plot(modFit$finalModel, )
text(modFit$finalModel, use.n = T, all=T, cex=.8)
plot(modFit$finalModel, uniform = T, main = "Classification Tree")
text(modFit$finalModel, use.n = T, all=T, cex=.8)
install.packadges("rattle")
install.packages("rattle")
require(rattle)
fancyRpartPlot(modFit$finalModel)
install.packages("rpart.plot")
require(rpart.plot)
fancyRpartPlot(modFit$finalModel)
plot(modFit$finalModel, uniform = T, main = "Classification Tree")
text(modFit$finalModel, use.n = T, all=T, cex=.8)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
training <- segmentationOriginal[which(segmentationOriginal$Case == "Train"),]
testing <- segmentationOriginal[which(segmentationOriginal$Case == "Test"),]
modFit <- train(Class~., method = "rpart", data=training)
modFit$finalModel
plot(modFit, uniform=TRUE,
main="Classification Tree")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
plot(modFit, uniform=TRUE,
main="Classification Tree")
text(modFit, use.n=TRUE, all=TRUE, cex=.8)
plot(modFit$finalModel, uniform=TRUE,
main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
fancyRpartPlot(modFit$finalModel)
modFit <- train(Class~., method = "rpart", data=training, tuneLength = 5)
plot(modFit$finalModel, uniform=TRUE,
main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
modFit <- train(Class~., method = "rpart", data=training, tuneLength = 6)
plot(modFit$finalModel, uniform=TRUE,
main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
modFit <- train(Class~., method = "rpart", data=training, tuneLength = 8)
plot(modFit$finalModel, uniform=TRUE,
main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
modFit <- train(Class~., method = "rpart", data=training, tuneLength = 6)
plot(modFit$finalModel, uniform=TRUE,
main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
fit <- rpart(Class~.,data = training, )
plot(fit, uniform=TRUE,
main="Classification Tree")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
training <- training[,-2]
fit <- rpart(Class~.,data = training, )
plot(fit, uniform=TRUE,
main="Classification Tree")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
library(pgmm)
data(olive)
olive = olive[,-1]
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
fit <- train(chd ~ age + alcohol + obesity + tabacco + typea + ldl, data = trainSA, method = "glm", family = "binomial")
library(caret)
fit <- train(chd ~ age + alcohol + obesity + tabacco + typea + ldl, data = trainSA, method = "glm", family = "binomial")
fit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, data = trainSA, method = "glm", family = "binomial")
plot(fit)
plot(fit$finalModel)
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(trainSA$chd, predict(fit, newdata=testSA))
missClass(testSA$chd, predict(fit, newdata=testSA))
missClass(train$chd, predict(fit, newdata=trainSA))
missClass(trainSA$chd, predict(fit, newdata=trainSA))
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.test$y <- as.factor(vowel.test$y)
vowel.train$y <- as.factor(vowel.train$y)
set.seed(33833)
library(caret)
fit <- train(y~., data = vowel.train, method = "rf", prox=TRUE)
rfNews()
fit <- train(y~., data = vowel.train, method = "rf", prox=TRUE)
fit
varImp(fit, data=vowel.train)
library(caret)
version()
install.packages(c("curl", "ElemStatLearn", "googleVis", "httr", "lme4", "plyr", "R6", "RCurl", "rpart", "scales", "stringi", "XML"))
install.packages("dplyr")
install.packages("readr")
library(caret)
library(kernlab)
data(spam)
View(spam)
?spam
inTrain <- createDataPartition(y=spam$tipe, p = 0.75, list = F)
inTrain <- createDataPartition(y=spam$type, p = 0.75, list = F)
inTrainL <- createDataPartition(y=spam$type, p = 0.75, list = T)
trianing <- spam[inTrain]
testing <- spam[-inTrain]
trianing <- spam[inTrain,]
testing <- spam[-inTrain,]
set.seed(32323)
fold <- createFolds(spam$type, k = 10, list = T)
fold <- createFolds(spam$type, k = 10, list = T)
fold[[1]]
names(fold)
folds <- createFolds(spam$type, k = 10, list = T, returnTrain = T)
folds <- createFolds(spam$type, k = 10, list = F, returnTrain = T)
folds <- createFolds(spam$type, k = 10, list = T, returnTrain = T)
set.seed(32323)
folds <- createFolds(spam$type, k = 10, list = T, returnTrain = T)
names (folds)
set.seed(32323)
resample.folds <- createResample(spam$type, times = 10, list = T)
set.seed(32323)
tme <- 1:1000
timeSlice <- createTimeSlices(y = tme, initialWindow = 20, horizon = 1)
names(timeSlice)
timeSlice$train
summary(timeSlice$test)
set.seed(32323)
tme <- 1:1000
timeSlice <- createTimeSlices(y = tme, initialWindow = 20, horizon = 10)
source('D:/Coursera/Data Scientist Specialization/8 Practical Machine Learning/Week 2/Week 2 Script.R')
source('D:/Coursera/Data Scientist Specialization/8 Practical Machine Learning/Week 2/Week 2 Script.R')
source('D:/Coursera/Data Scientist Specialization/8 Practical Machine Learning/Week 2/Week 2 Script.R')
source('D:/Coursera/Data Scientist Specialization/8 Practical Machine Learning/Week 2/Week 2 Script.R')
source('D:/Coursera/Data Scientist Specialization/8 Practical Machine Learning/Week 2/Week 2 Script.R')
modelFit <- train(type ~ ., data = training, method = "glm")
source('D:/Coursera/Data Scientist Specialization/8 Practical Machine Learning/Week 2/Week 2 Script.R')
warnings
warnings()
modelFit <- train(type ~ ., data = training, method = "glm")
modelFit$finalModel
plot(modelFit)
plot(modelFit$finalModel)
plot(modelFit$finalModel)
set.seed(1235)
modelFit <- train(type ~ ., data = training, method = "glm")
modelFit
library(caret)
library(kernlab)
source('D:/Coursera/Data Scientist Specialization/8 Practical Machine Learning/Week 2/Week 2 Script.R')
modelFit
plot(modelFit)
plot(modelFit)
plot(modelFit$finalModel)
source('D:/Coursera/Data Scientist Specialization/8 Practical Machine Learning/Week 2/Week 2 Script.R')
source('D:/Coursera/Data Scientist Specialization/8 Practical Machine Learning/Week 2/Week 2 Script.R')
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
IL.pred.index <- grep("^IL", names(training))
names(training)[IL.pred.index]
PP <- training[,IL.pred.index]
preProc <- preProcess(PP, method = "pca", thresh = .90)
preProc <- preProcess(PP, method = "pca", thresh = .80)
preProc$thresh
preProc$thresh
preProc$numComp
preProc$knnSummary
rm(list = ls())
ls()
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
IL.pred.index <- grep("^IL", names(training))
names(training)[IL.pred.index]
PP <- training[,IL.pred.index]
preProc <- preProcess(PP, method = "pca", thresh = .80)
preProc$numComp
ls()
rm(list = ls())
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
PCpreProc <-
1
IL.pred.index <- grep("^IL", names(training))
names(training)[IL.pred.index]
IL.training <- training[,IL.pred.index]
head(diagnosis)
length(diagnosis)
View(training)
IL.training <- training[,c(1, IL.pred.index)]
View(IL.training)
fitModel <- train(diagnosis~., method = "glm", data = IL.training)
op <- predict(fitModel, data = testing)
PCAfitModel <- train(diagnosis~., method = "glm", preProcess(methods = "pca", thresh = .80), data = IL.training)
PCAfitModel <- train(diagnosis~., method = "glm", preProcess ="pca", thresh = .80, data = IL.training)
PCAfitModel <- train(diagnosis~., method = "glm", preProcess ="pca", data = IL.training)
PCAfitModel <- train(diagnosis~., method = "glm", preProcess = c("pca", .80), data = IL.training)
args(train.default)
args(trainControl)
PCAfitModel <- train(diagnosis~., method = "glm", preProcess ="pca", trControl = trinControl(preProcOptions = list(thresh = 0.8,
ICAcomp = 3, k = 5)), data = IL.training)
PCAfitModel <- train(diagnosis~., method = "glm", preProcess ="pca", trControl = trainControl(preProcOptions = list(thresh = 0.8,
ICAcomp = 3, k = 5)), data = IL.training)
plot(fitModel$finalModel)
plot(fitModel$finalModel)
plot(PCAfitModel$finalModel)
plot(PCAfitModel$finalModel, pch = 19, cex = 0.5, col = '#00000010')
plot(fitModel$finalModel, pch = 19, cex = 0.5, col = '#00000010')
pred <- predict(fitModel, testing)
length(pred)
pred <- predict(fitModel, testing)
predPCA <- predict(PCAfitModel, testing)
qplot(diagnosis, pred, data = testing)
table(pred, testing$diagnosis)
table(predPCA, testing$diagnosis)
mean(predPCA == testing$diagnosis)
mean(pred == testing$diagnosis)
rm(list=ls())
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
#Solution
IL.pred.index <- grep("^IL", names(training))
names(training)[IL.pred.index]
IL.training <- training[,c(1, IL.pred.index)]
# Non-PCA model fit
fitModel <- train(diagnosis~., method = "glm", data = IL.training)
#PCA model fit
PCAfitModel <- train(diagnosis~., method = "glm", preProcess ="pca",
trControl = trainControl(preProcOptions = list(thresh = 0.8, ICAcomp = 3, k = 5)),
data = IL.training)
pred <- predict(fitModel, testing)
table(pred, testing$diagnosis)
mean(pred == testing$diagnosis)
predPCA <- predict(PCAfitModel, testing)
table(predPCA, testing$diagnosis)
mean(predPCA == testing$diagnosis)
setwd("D:/Coursera/Data Scientist Specialization/8 Practical Machine Learning/Course Project/answers")
readRDS("modFitRF_75_training_500_trees.rds")
modFitRF <- readRDS("modFitRF_75_training_500_trees.rds")
library(caret)
library(readr)
# Config ----
na.tresh <- 100   # max missing values in a collumn to be considered usefull
train.part <- .75
# Functions ----
cleanData <- function(df = data.frame(), na.tresh = numeric()){
# Identify and remove columns with too many NA values
cleanCol <- colSums(is.na(df))
cleanCol <- which(cleanCol < na.tresh)
df <- df[,cleanCol]
# Identify and remove columns with character value
cleanCol <- lapply (names(df),
function (x) {typeof(df[,x])})
cleanCol <- which(cleanCol != "character")
# choose only numeric values and the class (last column)
df <- df[,c(cleanCol, length(df))]
# Remove timestamps, window number and row number
df <- df[, -(1:4)]
df
}
# Read and clean the data ----
rawTrainingData <- read_csv("pml-training.csv")
rawTrainingData <- cleanData(rawTrainingData, na.tresh)
ifelse (sum(is.na(rawTrainingData)),
"There are missing vaues in the training set!",
"There are NOT missing vaues in the training set!"
)
# Training the model ----
tc <- trainControl(## 10-fold CV
method = "repeatedcv",
number = 10,
## repeated ten times
repeats = 10)
set.seed(232323)
inTraining <- createDataPartition(rawTrainingData$classe, p = train.part, list = FALSE)
training   <- rawTrainingData[inTraining, ]
validating <- rawTrainingData[-inTraining, ]
#Exploratory analysis
ggplot(validating, aes(x = roll_belt, y = pitch_belt, color = classe)) + geom_point()
set.seed(232323)
# start.train <- Sys.time()
# modFitRF   <- train(as.factor(classe) ~ ., method="rf",
#                                 data = training, trainControl=tc)
# end.train <- Sys.time()
# train.dur <- end.train - start.train
modFitRF <- readRDS("modFitRF_75_training_500_trees.rds")
# Predicting on the training set ----
trainPred    <- predict(modFitRF, training[,-length(training)])
trainCorrect <- trainPred == training$classe
trainAcc     <- sum(trainCorrect)/length(training$classe)
table(training$classe, trainPred)
confusionMatrix(training$classe, trainPred)
# Predicting on the validating set
validPred    <- predict(modFitRF, validating[,-length(validating)])
validCorrect <- validPred == validating$classe
validAcc     <- sum(validCorrect)/length(validating$classe)
table(validating$classe, validPred)
confusionMatrix(validating$classe, validPred)
validWrong <- dim(validating)[1] - sum(validCorrect)
validAccuracy <- sum(validCorrect)/dim(validating)[1]
# Predicting on the validating set ----
rawTestingData  <- read_csv("pml-testing.csv")
testing         <- rawTestingData[,names(training[, -length(training)])]
testing$testPred <- predict(modFitRF, testing)
setwd("D:/Coursera/Data Scientist Specialization/8 Practical Machine Learning/Course Project")
library(caret)
library(readr)
# Config ----
na.tresh <- 100   # max missing values in a collumn to be considered usefull
train.part <- .75
# Functions ----
cleanData <- function(df = data.frame(), na.tresh = numeric()){
# Identify and remove columns with too many NA values
cleanCol <- colSums(is.na(df))
cleanCol <- which(cleanCol < na.tresh)
df <- df[,cleanCol]
# Identify and remove columns with character value
cleanCol <- lapply (names(df),
function (x) {typeof(df[,x])})
cleanCol <- which(cleanCol != "character")
# choose only numeric values and the class (last column)
df <- df[,c(cleanCol, length(df))]
# Remove timestamps, window number and row number
df <- df[, -(1:4)]
df
}
# Read and clean the data ----
rawTrainingData <- read_csv("pml-training.csv")
rawTrainingData <- cleanData(rawTrainingData, na.tresh)
ifelse (sum(is.na(rawTrainingData)),
"There are missing vaues in the training set!",
"There are NOT missing vaues in the training set!"
)
# Training the model ----
tc <- trainControl(## 10-fold CV
method = "repeatedcv",
number = 10,
## repeated ten times
repeats = 10)
set.seed(232323)
inTraining <- createDataPartition(rawTrainingData$classe, p = train.part, list = FALSE)
training   <- rawTrainingData[inTraining, ]
validating <- rawTrainingData[-inTraining, ]
#Exploratory analysis
ggplot(validating, aes(x = roll_belt, y = pitch_belt, color = classe)) + geom_point()
set.seed(232323)
# start.train <- Sys.time()
# modFitRF   <- train(as.factor(classe) ~ ., method="rf",
#                                 data = training, trainControl=tc)
# end.train <- Sys.time()
# train.dur <- end.train - start.train
modFitRF <- readRDS("modFitRF_75_training_500_trees.rds")
# Predicting on the training set ----
trainPred    <- predict(modFitRF, training[,-length(training)])
trainCorrect <- trainPred == training$classe
trainAcc     <- sum(trainCorrect)/length(training$classe)
table(training$classe, trainPred)
confusionMatrix(training$classe, trainPred)
# Predicting on the validating set
validPred    <- predict(modFitRF, validating[,-length(validating)])
validCorrect <- validPred == validating$classe
validAcc     <- sum(validCorrect)/length(validating$classe)
table(validating$classe, validPred)
confusionMatrix(validating$classe, validPred)
validWrong <- dim(validating)[1] - sum(validCorrect)
validAccuracy <- sum(validCorrect)/dim(validating)[1]
# Predicting on the validating set ----
rawTestingData  <- read_csv("pml-testing.csv")
testing         <- rawTestingData[,names(training[, -length(training)])]
testing$testPred <- predict(modFitRF, testing)
modFitRF
trainAcc
validAcc
0.25*19000
0.25*19000 *99.61
0.25*19000 *.9961
plot(modFitRF$finalModel)
library(caret)
library(readr)
# Config ----
na.tresh <- 100   # max missing values in a collumn to be considered usefull
train.part <- .75
# Functions ----
cleanData <- function(df = data.frame(), na.tresh = numeric()){
# Identify and remove columns with too many NA values
cleanCol <- colSums(is.na(df))
cleanCol <- which(cleanCol < na.tresh)
df <- df[,cleanCol]
# Identify and remove columns with character value
cleanCol <- lapply (names(df),
function (x) {typeof(df[,x])})
cleanCol <- which(cleanCol != "character")
# choose only numeric values and the class (last column)
df <- df[,c(cleanCol, length(df))]
# Remove timestamps, window number and row number
df <- df[, -(1:4)]
df
}
# Read and clean the data ----
rawTrainingData <- read_csv("pml-training.csv")
rawTrainingData <- cleanData(rawTrainingData, na.tresh)
ifelse (sum(is.na(rawTrainingData)),
"There are missing vaues in the training set!",
"There are NOT missing vaues in the training set!"
)
# Training the model ----
tc <- trainControl(## 10-fold CV
method = "repeatedcv",
number = 10,
## repeated ten times
repeats = 10)
set.seed(232323)
inTraining <- createDataPartition(rawTrainingData$classe, p = train.part, list = FALSE)
training   <- rawTrainingData[inTraining, ]
validating <- rawTrainingData[-inTraining, ]
#Exploratory analysis
ggplot(validating, aes(x = roll_belt, y = pitch_belt, color = classe)) + geom_point()
set.seed(232323)
# start.train <- Sys.time()
# modFitRF   <- train(as.factor(classe) ~ ., method="rf",
#                                 data = training, trainControl=tc)
# end.train <- Sys.time()
# train.dur <- end.train - start.train
modFitRF <- readRDS("modFitRF_75_training_500_trees.rds")
# Predicting on the training set ----
trainPred    <- predict(modFitRF, training[,-length(training)])
trainCorrect <- trainPred == training$classe
trainAcc     <- sum(trainCorrect)/length(training$classe)
table(training$classe, trainPred)
confusionMatrix(training$classe, trainPred)
# Predicting on the validating set
validPred    <- predict(modFitRF, validating[,-length(validating)])
validCorrect <- validPred == validating$classe
validAcc     <- sum(validCorrect)/length(validating$classe)
table(validating$classe, validPred)
confusionMatrix(validating$classe, validPred)
validWrong <- dim(validating)[1] - sum(validCorrect)
validAccuracy <- sum(validCorrect)/dim(validating)[1]
# Predicting on the validating set ----
rawTestingData  <- read_csv("pml-testing.csv")
testing         <- rawTestingData[,names(training[, -length(training)])]
testing$testPred <- predict(modFitRF, testing)
plot(modFitRF$finalModel)
plot(modFitRF)
plot(modFitRF$finalModel)
modFitRF$finalModel
modFitRF$finalModel$ntree
str(modFitRF$finalModel)
attr(plot(modFitRF$finalModel))
