source('D:/Coursera/Data Scientist Specialization/8 Practical Machine Learning/Course Project/Practical machine learning project.R')
source('D:/Coursera/Data Scientist Specialization/8 Practical Machine Learning/Course Project/Practical machine learning project.R')
source('D:/Coursera/Data Scientist Specialization/8 Practical Machine Learning/Course Project/Practical machine learning project.R')
sum(is.na(rawTrainingData))
ifelse (sum(is.na(rawTrainingData)),
print("There are missing vaues in the training set!"),
print("There are NOT missing vaues in the training set!")
)
ifelse (sum(is.na(rawTrainingData)),
print("There are missing vaues in the training set!"),
print("There are NOT missing vaues in the training set!")
)
print("There are NOT missing vaues in the training set!")
ifelse (sum(is.na(rawTrainingData)),
"There are missing vaues in the training set!",
"There are NOT missing vaues in the training set!"
)
ifelse (sum(is.na(rawTrainingData)),
"There are missing vaues in the training set!",
{
"There are NOT missing vaues in the training set!"
training <- rawTrainingData
}
)
cl
na.tresh <- 100  # max missing values in a collumn to be considered usefull
# Functions ----
cleanData <- function(df = data.frame(), na.tresh = numeric()){
# Identify and remove columns with too many NA values
cleanCol <- colSums(is.na(df))
cleanCol <- which(cleanCol < na.tresh)
df <- df[,cleanCol]
# Identify and remove columns with character value
cleanCol <- lapply (names(df),
function (x) {typeof(df[,x])})
cleanCol <- which(cleanCol != "character")
# choose only numeric values and the class (last column)
df <- df[,c(cleanCol, length(df))]
# Remove timestamps, window number and row number
df <- df[, -(1:4)]
df
}
# Read the data ----
rawTrainingData <- read_csv("pml-training.csv")
rawTrainingData <- cleanData(rawTrainingData, na.tresh)
ifelse (sum(is.na(rawTrainingData)),
"There are missing vaues in the training set!",
{
"There are NOT missing vaues in the training set!"
training <- rawTrainingData
}
)
ifelse (sum(is.na(rawTrainingData)),
"There are missing vaues in the training set!",
{
training <- rawTrainingData
"There are NOT missing vaues in the training set!"
}
)
tc <- trainControl(method="cv", number=3)
modFitRF <- train(as.factor(classe) ~ ., method="rf",data=training, trainControl=tc)
mem.limits()
memory.limit()
library(caret)
library(readr)
# Config ----
na.tresh <- 100  # max missing values in a collumn to be considered usefull
# Functions ----
cleanData <- function(df = data.frame(), na.tresh = numeric()){
# Identify and remove columns with too many NA values
cleanCol <- colSums(is.na(df))
cleanCol <- which(cleanCol < na.tresh)
df <- df[,cleanCol]
# Identify and remove columns with character value
cleanCol <- lapply (names(df),
function (x) {typeof(df[,x])})
cleanCol <- which(cleanCol != "character")
# choose only numeric values and the class (last column)
df <- df[,c(cleanCol, length(df))]
# Remove timestamps, window number and row number
df <- df[, -(1:4)]
df
}
# Read the data ----
rawTrainingData <- read_csv("pml-training.csv")
rawTrainingData <- cleanData(rawTrainingData, na.tresh)
ifelse (sum(is.na(rawTrainingData)),
"There are missing vaues in the training set!",
{       training <- rawTrainingData
"There are NOT missing vaues in the training set!"
}
)
tc <- trainControl(method="cv", number=3)
inTraining <- createDataPartition(rawTrainingData, p = 1/4, list = FALSE)
inTraining <- createDataPartition(rawTrainingData, p = .5, list = FALSE)
inTraining <- createDataPartition(rawTrainingData$classe, p = .5, list = FALSE)
inTraining <- createDataPartition(rawTrainingData$classe, p = .25, list = FALSE)
training <- rawTrainingData[inTraining, ]
validating <- rawTrainingData[-inTraining, ]
inTraining <- createDataPartition(rawTrainingData$classe, p = .10, list = FALSE)
training <- rawTrainingData[inTraining, ]
validating <- rawTrainingData[-inTraining, ]
modFitRF <- train(as.factor(classe) ~ ., method="rf",
data=training, trainControl=tc)
plot(modFitRF)
plot(modFitRF$finalModel)
modFitRF$finalModel
modFitRF
modFitRF$xlevels
modFitRF$method
modFitRF$pred
modFitRF$preProcess
pred(modFitRF, training)
pred <- predict(modFitRF, training)
op <- pred == training[,"classe"]
sum(op)
pred <- predict(modFitRF, training[,-length(training)])
op <- pred == training$classe
sum(op)
pred
training$classe
sum(op)
validpred <- predict(modFitRF, validating[, -length(validating)])
validop <- validpred == validating$classe
sum(validop)
sum(validop)/ length(validating)
sum(validop)/ length(validating$classe)
modFitRF
rawTestingData <- read_csv("pml-testing.csv")
rawTestingData <- cleanData(rawTestingData, na.tresh)
rawTestingData <- cleanData(rawTestingData, 5)
rawTestingData <- read_csv("pml-testing.csv")
rawTestingData <- cleanData(rawTestingData, 10)
rawTestingData <- read_csv("pml-testing.csv")
testing <- rawTestingData[,names(training)]
which(names(training)%in%names(rawTestingData))
testing <- rawTestingData[,names(training[, -length(training)])]
modFitRF
trainPred <- predict(modFitRF, training[,-length(training)])
trainCorrect <- trainPred == training$classe
trainAcc <- sum(trainCorrect)/length(training$classe)
trainPred <- predict(modFitRF, training[,-length(training)])
trainCorrect <- trainPred == training$classe
trainAcc <- sum(trainCorrect)/length(training$classe)
# Predicting on the validating set
validPred <- predict(modFitRF, validating[,-length(validating)])
validCorrect <- trainPred == validating$classe
validAcc <- sum(validCorrect)/length(validating$classe)
sum(validCorrect)
validPred <- predict(modFitRF, validating[,-length(validating)])
validCorrect <- validPred == validating$classe
validAcc <- sum(validCorrect)/length(validating$classe)
rawTestingData <- read_csv("pml-testing.csv")
testing <- rawTestingData[,names(training[, -length(training)])]
testing$testPred <- predict(modFitRF, testing)
View(testing)
table(training$classe, trainPred)
table(validating$classe, validPred)
modFitRF$fin1
modFitRF$finalModel
plot(modFitRF)
modFitRF
modFitRF$finalModel
plot(modFitRF$finalModel)
ggplot(modFitRF$finalModel)
qplot(modFitRF$finalModel)
featurePlot(training[,-length(training)], training[,length(training)], scatter )
featurePlot(training[,-length(training)], training[,length(training)], plot = "scatter" )
warning()
warnings()
featurePlot(training[,-length(training)], training[,length(training)], plot = "scatter" )
warnings()
ggplot(training, roll_belt, color = classesToAM() )
ggplot(training, roll_belt, color = classe )
ggplot(training, aes(x = roll_belt, y =pitch_belt, color = classe), geom = "geom_point")
ggplot(training, aes(x = roll_belt, y =pitch_belt, color = classe)) + geom_point()
ggplot(training, aes(x = roll_belt, color = classe)) + geom_point()
ggplot(training, aes(x = roll_belt, y = yaw_belt  color = classe)) + geom_point()
ggplot(training, aes(x = roll_belt, y = yaw_belt,  color = classe)) + geom_point()
ggplot(training, aes(x = pitch_belt, y = yaw_belt,  color = classe)) + geom_point()
ggplot(training, aes(x = pitch_belt, y = roll_belt,  color = classe)) + geom_point()
nzv <- nearZeroVar(training, saveMetrics = T)
nzv
ggplot(training, aes(x = roll_dumbbell , y = pitch_dumbbell ,  color = classe)) + geom_point()
ggplot(training, aes(x = pitch_dumbbell , y = roll_dumbbell ,  color = classe)) + geom_point()
ggplot(training, aes(x = pitch_dumbbell , y = yaw_dumbbell ,  color = classe)) + geom_point()
ggplot(training, aes(x = roll_dumbbell , y = yaw_dumbbell ,  color = classe)) + geom_point()
ggplot(training, aes(x = pitch_dumbbell , y = roll_dumbbell ,  color = classe)) + geom_point()
ggplot(training, aes(x = roll_dumbbell , y = yaw_dumbbell ,  color = classe)) + geom_point()
ggplot(training, aes(x = pitch_dumbbell , y = yaw_dumbbell ,  color = classe)) + geom_point()
ggplot(training, aes(x = roll_arm , y = pitch_arm ,  color = classe)) + geom_point()
ggplot(training, aes(x = roll_arm , y = yaw_arm ,  color = classe)) + geom_point()
ggplot(training, aes(x = pitch_arm , y = yaw_arm ,  color = classe)) + geom_point()
ggplot(validating, aes(x = pitch_arm , y = yaw_arm ,  color = classe)) + geom_point()
ggplot(validating, aes(x = pitch_arm , y = roll_arm ,  color = classe)) + geom_point()
ggplot(validating, aes(x = pitch_belt , y = roll_belt ,  color = classe)) + geom_point()
ggplot(validating, aes(x = pitch_belt , y = yaw_belt ,  color = classe)) + geom_point()
ggplot(validating, aes(x = roll_belt , y = yaw_belt ,  color = classe)) + geom_point()
ggplot(validating[classe==c("A","B","C"),], aes(x = roll_belt , y = yaw_belt ,  color = classe)) + geom_point()
ggplot(validating[which(validating$classe==c("A","B","C")),], aes(x = roll_belt , y = yaw_belt ,  color = classe)) + geom_point()
ggplot(validating[which(validating$classe==c("A","B","C")),], aes(x = roll_belt , y = pitch_belt ,  color = classe)) + geom_point()
ggplot(validating[which(validating$classe==c("A","B","C")),], aes(x = roll_belt , y = pitch_belt ,  color = classe)) + geom_point()
ggplot(rawTrainingData[which(validating$classe==c("A","B","C")),], aes(x = roll_belt , y = pitch_belt ,  color = classe)) + geom_point()
ggplot(rawTrainingData, aes(x = roll_belt , y = pitch_belt ,  color = classe)) + geom_point()
validAcc     <- sum(validCorrect)/length(validating$classe)
validAcc
table(validating$classe, validPred)
sum(validCorrect)
dim(validating)
sum(validCorrect) - dim(validating)[1]
library(caret)
library(readr)
# Config ----
na.tresh <- 100   # max missing values in a collumn to be considered usefull
# Functions ----
cleanData <- function(df = data.frame(), na.tresh = numeric()){
# Identify and remove columns with too many NA values
cleanCol <- colSums(is.na(df))
cleanCol <- which(cleanCol < na.tresh)
df <- df[,cleanCol]
# Identify and remove columns with character value
cleanCol <- lapply (names(df),
function (x) {typeof(df[,x])})
cleanCol <- which(cleanCol != "character")
# choose only numeric values and the class (last column)
df <- df[,c(cleanCol, length(df))]
# Remove timestamps, window number and row number
df <- df[, -(1:4)]
df
}
# Read and clean the data ----
rawTrainingData <- read_csv("pml-training.csv")
rawTrainingData <- cleanData(rawTrainingData, na.tresh)
ifelse (sum(is.na(rawTrainingData)),
"There are missing vaues in the training set!",
"There are NOT missing vaues in the training set!"
)
# Training the model ----
tc <- trainControl(method="cv", number = 3)
set.seed(232323)
inTraining <- createDataPartition(rawTrainingData$classe, p = .10, list = FALSE)
training   <- rawTrainingData[inTraining, ]
validating <- rawTrainingData[-inTraining, ]
set.seed(232323)
modFitRF   <- train(as.factor(classe) ~ ., method="rf",
data=training, trainControl=tc)
library(caret)
library(readr)
# Config ----
na.tresh <- 100   # max missing values in a collumn to be considered usefull
# Functions ----
cleanData <- function(df = data.frame(), na.tresh = numeric()){
# Identify and remove columns with too many NA values
library(caret)
library(readr)
# Config ----
na.tresh <- 100   # max missing values in a collumn to be considered usefull
# Functions ----
cleanData <- function(df = data.frame(), na.tresh = numeric()){
# Identify and remove columns with too many NA values
cleanCol <- colSums(is.na(df))
cleanCol <- which(cleanCol < na.tresh)
df <- df[,cleanCol]
# Identify and remove columns with character value
cleanCol <- lapply (names(df),
function (x) {typeof(df[,x])})
cleanCol <- which(cleanCol != "character")
# choose only numeric values and the class (last column)
df <- df[,c(cleanCol, length(df))]
# Remove timestamps, window number and row number
df <- df[, -(1:4)]
df
}
# Read and clean the data ----
rawTrainingData <- read_csv("pml-training.csv")
rawTrainingData <- cleanData(rawTrainingData, na.tresh)
ifelse (sum(is.na(rawTrainingData)),
"There are missing vaues in the training set!",
"There are NOT missing vaues in the training set!"
)
# Training the model ----
tc <- trainControl(method="cv", number = 3)
set.seed(232323)
inTraining <- createDataPartition(rawTrainingData$classe, p = .10, list = FALSE)
training   <- rawTrainingData[inTraining, ]
validating <- rawTrainingData[-inTraining, ]
set.seed(232323)
start.train <- Sys.time()
modFitRF   <- train(as.factor(classe) ~ ., method="rf",
data = training, trainControl=tc)
library(caret)
library(readr)
# Config ----
na.tresh <- 100   # max missing values in a collumn to be considered usefull
# Functions ----
cleanData <- function(df = data.frame(), na.tresh = numeric()){
# Identify and remove columns with too many NA values
cleanCol <- colSums(is.na(df))
cleanCol <- which(cleanCol < na.tresh)
df <- df[,cleanCol]
# Identify and remove columns with character value
cleanCol <- lapply (names(df),
function (x) {typeof(df[,x])})
cleanCol <- which(cleanCol != "character")
# choose only numeric values and the class (last column)
df <- df[,c(cleanCol, length(df))]
# Remove timestamps, window number and row number
df <- df[, -(1:4)]
df
}
# Read and clean the data ----
rawTrainingData <- read_csv("pml-training.csv")
rawTrainingData <- cleanData(rawTrainingData, na.tresh)
ifelse (sum(is.na(rawTrainingData)),
"There are missing vaues in the training set!",
"There are NOT missing vaues in the training set!"
)
# Training the model ----
tc <- trainControl(method="cv", number = 3)
set.seed(232323)
inTraining <- createDataPartition(rawTrainingData$classe, p = .10, list = FALSE)
training   <- rawTrainingData[inTraining, ]
validating <- rawTrainingData[-inTraining, ]
set.seed(232323)
start.train <- Sys.time()
modFitRF   <- train(as.factor(classe) ~ ., method="rf",
data = training, trainControl=tc)
end.train <- Sys.time()
train.dur <- end.train - start.train
train.dur
trainPred    <- predict(modFitRF, training[,-length(training)])
trainCorrect <- trainPred == training$classe
trainAcc     <- sum(trainCorrect)/length(training$classe)
table(training$classe, trainPred)
# Predicting on the validating set
validPred    <- predict(modFitRF, validating[,-length(validating)])
validCorrect <- validPred == validating$classe
validAcc     <- sum(validCorrect)/length(validating$classe)
table(validating$classe, validPred)
validWrong <- dim(validating)[1] - validCorrect
validWrong <- dim(validating)[1] - sum(validCorrect)
validWrong <- dim(validating)[1] - sum(validCorrect)
validAccuracy <- validCorrect/dim(validating)[1]
# Predicting on the validating set ----
rawTestingData  <- read_csv("pml-testing.csv")
testing         <- rawTestingData[,names(training[, -length(training)])]
testing$testPred <- predict(modFitRF, testing)
testing$testPred
write_csv(testing, "RF_10_percent_training_cv_3.csv")
save
View(validating)
ggplot(validating, aes(x = roll_belt, y = pitch_belt, color = classe)) + geom_point()
View(validating)
train.dur
source('D:/Coursera/Data Scientist Specialization/8 Practical Machine Learning/Course Project/Practical machine learning project.R')
train.dur
table(training$classe, trainPred)
modFitRF
plot(modFitRF)
plot(modFitRF$finalModel)
modFitRF$finalModel
table(training$classe, trainPred)
table(validating$classe, validPred)
validAccuracy <- validCorrect/dim(validating)[1]
validAccuracy <- sum(validCorrect)/dim(validating)[1]
source('D:/Coursera/Data Scientist Specialization/8 Practical Machine Learning/Course Project/Practical machine learning project.R')
source('D:/Coursera/Data Scientist Specialization/8 Practical Machine Learning/Course Project/Practical machine learning project.R')
alarm()
alarm()
alarm()
alarm()
train.dur
alarm()
alarm()
\a
alrm()
alarm()
plot(modFitRF)
plot(modFitRF$finalModel)
modFitRF
modFitRF$finalModel
source('D:/Coursera/Data Scientist Specialization/8 Practical Machine Learning/Course Project/Practical machine learning project.R')
source('D:/Coursera/Data Scientist Specialization/8 Practical Machine Learning/Course Project/Practical machine learning project.R')
modFitRF
plot(modFitRF$finalModel)
train.dur
alarm()
train.dur
modFitRF$finalModel
confusionMatrix(validating$classe, validPred)
confusionMatrix(training$classe, trainPred)
confusionMatrix(validating$classe, validPred)
source('D:/Coursera/Data Scientist Specialization/8 Practical Machine Learning/Course Project/Practical machine learning project.R')
which(validpred == "A")
ggplot(validating[,classe == 'A'], aes(x = roll_belt, y = pitch_belt, color = validating[, classe=='A']== validPred["A"])) + geom_point()
ggplot(validating[,"classe" == 'A'], aes(x = roll_belt, y = pitch_belt, color = validating[, "classe" =='A']== validPred["A"])) + geom_point()
ggplot(validating[,"classe" == 'A'], aes(x = roll_belt, y = pitch_belt, color = validating[, "classe" =='A']== levels(validPred) =="A")) + geom_point()
ggplot(validating[,"classe" == "A"], aes(x = roll_belt, y = pitch_belt, color = validating[, "classe" =="A"] == (levels(validPred) =="A"))) + geom_point()
op <- validating[,"classe"=="A"]
View(op)
op <- validating[,which("classe"=="A")]
op <- validating[, validating$classe =="A")]
op <- validating[, validating$classe =="A"]
op <- validating[validating$classe =="A",]
op <- validating[which(validating$classe =="A"), validating$classe]
op <- validating[which(validating$classe =="A"), "classe"]
op <- which (validating[which(validating$classe =="A"), "classe"] != validPred == "A")
op <- which (validating[which(validating$classe =="A"), "classe"] != levels(validPred) == "A")
op <- which (validating[which(validating$classe =="A"), "classe"] != which(levels(validPred) == "A"))
which(levels(validPred) == "A")
which(validPred== "A")
op <- which (validating[which(validating$classe =="A"), "classe"] != which(validPred == "A")
)
ggplot(validating[,"classe" == "A"], aes(x = roll_belt, y = pitch_belt, color = validdCorrect)) + geom_point()
ggplot(validating["classe" == "A","classe"], aes(x = roll_belt, y = pitch_belt, color = validdCorrect)) + geom_point()
ggplot(validating["classe" == "A",], aes(x = roll_belt, y = pitch_belt, color = validdCorrect)) + geom_point()
ggplot(validating["classe" == "A",], aes(x = roll_belt, y = pitch_belt, color = validCorrect)) + geom_point()
confusionMatrixi(training$classe, trainPred)
confusionMatrix(training$classe, trainPred)
confusionMatrix(validating$classe, validPred)
getTree(modFitRF$finalModel)
getTree(modFitRF$finalModel, k=2)
getTree(modFitRF$finalModel, k=1)
getTree(modFitRF$finalModel, k=3)
getTree(modFitRF$finalModel, k=16)
getTree(modFitRF$finalModel, k=500)
getTree(modFitRF$finalModel, k=501)
answers(testing$testPred)
answers <- testing$testPred
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
setwd("D:/Coursera/Data Scientist Specialization/8 Practical Machine Learning/Course Project/answers")
pml_write_files(answers)
answers
saveRDS(modFitRF, "modFitRF_75_training_500_trees.rds")
op <- readRDS("modFitRF_75_training_500_trees.rds")
predict(op, testing)
